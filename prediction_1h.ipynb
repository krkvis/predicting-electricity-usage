{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of recursive neural network for electricity usage prediction 1 hour ahead\n",
    "Applied on case study Oslo Airport Gardermoen for Kjersti Rustad Kvisberg's master thesis autumn 2022.\n",
    "\n",
    "Requires input with minimum timestamp of electricity measurement and electricity measurement to predict  electricity usage one hour ahead. \n",
    "For this master thesis, weather and passenger measurements and calendar information were also used as explanatory variables.\n",
    "\n",
    "Data is saved in Pandas dataframes, and models are built using Tensorflow and Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T11:32:04.233218Z",
     "start_time": "2021-01-08T11:32:04.230702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error\n",
    "from keras.layers import Input, Dense, LSTM, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 221 # customized to use case\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set into dataframe using first column as index\n",
    "df = pd.read_csv('results/data_output/data_cleaned_2017_2021_dummy.csv', \n",
    "                 index_col = 0) # customized to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time stamp index to datetime object for further work\n",
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting data\n",
    "Dataframe is checked for missing data (NaN values). Descritive statistics are computed and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print overview of the dataframe to see if there is missing data\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Dataframe is filtered on relevant time period (April through September 2019).  \n",
    "Training data are split into target and feature sets, and timesplits with training and validation data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select time period\n",
    "df2 = df.loc[df.index >= '2019-04-01 00:00:00']\n",
    "df3 = df2.loc[df2.index <= '2019-09-30 23:00:00']\n",
    "\n",
    "# Specify hourly frequency of measurements\n",
    "df3 = df3.asfreq('1H')\n",
    "\n",
    "# Print info about data\n",
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics about numerical columns in df\n",
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target column and reshape into correct form 1D array\n",
    "df_y = df3['Timesverdi'].copy()\n",
    "y = df_y.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of target array\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns of dataframe\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with only relevant columns\n",
    "df_x = df3.drop(['Timesverdi', 'Dato', 'index', 'Ã…r' # customized to use case\n",
    "    ],axis=1)\n",
    "\n",
    "# Create feature matrix as values from dataframe\n",
    "df_x = df_x.iloc[:,:]\n",
    "x = df_x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns of dataframe with only relevant columns\n",
    "df_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of feature matrix\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stats\n",
    "num_features = x.shape[1]\n",
    "num_targets = y.shape[1]\n",
    "num_obs = len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify properties of train, validation and test split and lookback\n",
    "TRAIN_SIZE = 0.80 # customized to use case\n",
    "VAL_SIZE = 0.10 # customized to use case\n",
    "TEST_SIZE = 1 - TRAIN_SIZE - VAL_SIZE\n",
    "num_train = int(num_obs * TRAIN_SIZE)\n",
    "num_val = int(num_obs * VAL_SIZE)\n",
    "num_test = num_obs - num_train - num_val\n",
    "\n",
    "N_LOOKBACK = 24*7  # length of timestep dimension in Keras for training batches # customized to use case\n",
    "\n",
    "BATCH_SIZE_TRAIN = 32 # customized to use case\n",
    "BATCH_SIZE_TEST = 1 # customized to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new feature matrices and target arrays for test, validation and training data\n",
    "\n",
    "# First num_train objects are training data\n",
    "x_train = x[0:num_train]\n",
    "# Taking into account lookback, the next num_val objects are validation data\n",
    "x_val = x[num_train - N_LOOKBACK:num_train + num_val] \n",
    "# Taking into account lookback, the rest are testing data\n",
    "x_test = x[num_train + num_val - N_LOOKBACK:]\n",
    "\n",
    "# Similarly for target arrays\n",
    "y_train_list = y[0:num_train]\n",
    "y_val_list = y[num_train - N_LOOKBACK:num_train + num_val]\n",
    "y_test_list = y[num_train + num_val - N_LOOKBACK:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save time stamps in series for later plotting\n",
    "train_dt = df3.iloc[0:num_train].index\n",
    "val_dt = df3.iloc[num_train:num_train + num_val].index\n",
    "test_dt = df3.iloc[num_train + num_val - N_LOOKBACK:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler objects and fit them to training data\n",
    "x_scaler = MinMaxScaler()\n",
    "x_scaler = x_scaler.fit(x_train)\n",
    "\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaler = y_scaler.fit(y_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform feature matrices and target arrays with scalers\n",
    "x_train_scaled = x_scaler.transform(x_train)\n",
    "x_val_scaled = x_scaler.transform(x_val)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_train_list_scaled = y_scaler.transform(y_train_list)\n",
    "y_val_list_scaled = y_scaler.transform(y_val_list)\n",
    "y_test_list_scaled = y_scaler.transform(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check lengths\n",
    "len(x_train_scaled) + len(x_val_scaled) + len(x_test_scaled) > len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute baseline scores\n",
    "A \"dumb\" model is created using Scikit-learns DummyRegressor. The model predicts each measurement to be the mean value of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dumb model\n",
    "dummy_regr = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Fit to training data\n",
    "dummy_regr.fit(x_train_scaled, y_train_list_scaled) \n",
    "\n",
    "# Predict on test data\n",
    "dummy_pred = dummy_regr.predict(x_test_scaled) \n",
    "\n",
    "# Save measured values in new array for comparison\n",
    "dummy_true = y_test_list_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform predictions and measured values back to non-scaled values\n",
    "dummy_true = y_scaler.inverse_transform(dummy_true.reshape(-1, 1))\n",
    "dummy_pred = y_scaler.inverse_transform(dummy_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print scores\n",
    "print('MSE = ', mean_squared_error(dummy_true, dummy_pred))\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(dummy_true, dummy_pred)))\n",
    "print('MAE = ', mean_absolute_error(dummy_true, dummy_pred))\n",
    "print('MAPE = ', mean_absolute_percentage_error(dummy_true, dummy_pred)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and fit neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model arcitecture\n",
    "USE_DROPOUTLAYER = True # customized to use case\n",
    "DROPOUT_SHARE = 0.0 # customized to use case\n",
    "N_UNITS = 32 # customized to use case\n",
    "N_EPOCHS = 3*num_features\n",
    "ACTIVATION = 'ReLU'  # some other options: ['tanh', ' linear ', 'ReLU ']\n",
    "NUM_LAYERS = 1 # customized to use case\n",
    "ACTIVATIONS = 'default'\n",
    "\n",
    "def build_model():\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # LSTM layer with prev. specified number of units and default activations\n",
    "    model.add(keras.layers.LSTM(units=N_UNITS,\n",
    "                    input_shape=(N_LOOKBACK, num_features),\n",
    "                    # return_sequences=True, # uncomment if >1 layer\n",
    "                    ))\n",
    "    \n",
    "    # Adds dropoutlayer with prev. specified percentace of dropout\n",
    "    if USE_DROPOUTLAYER:\n",
    "        model.add(keras.layers.Dropout(DROPOUT_SHARE, \n",
    "                                       seed=seed))\n",
    "    \n",
    "    # Addition of second layer with specified number of units\n",
    "    if NUM_LAYERS > 1:\n",
    "        model.add(keras.layers.LSTM(units=int(N_UNITS),\n",
    "                    # return_sequences=True # uncomment if >2 layers\n",
    "        ))\n",
    "\n",
    "        if USE_DROPOUTLAYER:\n",
    "            model.add(keras.layers.Dropout(DROPOUT_SHARE, seed=seed))\n",
    "    \n",
    "    # Addition of third layer with specified number of units\n",
    "    if NUM_LAYERS > 2:\n",
    "        model.add(keras.layers.LSTM(units=int(N_UNITS)))\n",
    "    \n",
    "        if USE_DROPOUTLAYER:\n",
    "            model.add(keras.layers.Dropout(DROPOUT_SHARE, seed=seed))\n",
    "    \n",
    "    # Add final Dense layer, optionally with not default (linear) activation\n",
    "    model.add(keras.layers.Dense(1, \n",
    "            # activation = ACTIVATION # customized to use case\n",
    "    ))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of model\n",
    "build_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare lists for saving scores and predictions\n",
    "scores = []\n",
    "train_predictions = []\n",
    "val_predictions = []\n",
    "test_predictions = []\n",
    "\n",
    "# Save current date and time to be used for model id\n",
    "date = pd.to_datetime('today').strftime(\"%d.%m.%Y %H.%M.%S\")\n",
    "MODEL_NAME = date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation choices\n",
    "LOSS = 'mse' # customized to use case\n",
    "METRICS = ['mse', tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae', 'mape'] # customized to use case\n",
    "OPTIMIZER = 'adam' # customized to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape scaled target arrays\n",
    "y_train_scaled = np.reshape(y_train_list_scaled[:, 0], newshape=(y_train_list_scaled.shape[0], 1))\n",
    "y_val_scaled = np.reshape(y_val_list_scaled[:, 0], newshape=(y_val_list_scaled.shape[0], 1))\n",
    "y_test_scaled = np.reshape(y_test_list_scaled[:, 0], newshape=(y_test_list_scaled.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training and prediction for process for model\n",
    "model = build_model()\n",
    "print(model)\n",
    "\n",
    "# Initialize TimeseriesGenerator from Keras for each part of data\n",
    "# (PS. deprecated tool, consider switching to generator recommendation in error message)\n",
    "# For training data\n",
    "train_data_gen_shuffle = TimeseriesGenerator(x_train_scaled, y_train_scaled, \n",
    "            length=N_LOOKBACK, sampling_rate=1, stride=1, \n",
    "            batch_size=BATCH_SIZE_TRAIN, \n",
    "            shuffle=True)\n",
    "\n",
    "# For prediction on training data\n",
    "train_data_gen = TimeseriesGenerator(x_train_scaled, y_train_scaled, \n",
    "            length=N_LOOKBACK, sampling_rate=1, stride=1, \n",
    "            batch_size=BATCH_SIZE_TRAIN, \n",
    "            shuffle=False)\n",
    "\n",
    "# For validation data during training\n",
    "val_data_gen = TimeseriesGenerator(x_val_scaled, y_val_scaled, \n",
    "            length=N_LOOKBACK, sampling_rate=1, stride=1, \n",
    "            batch_size=BATCH_SIZE_TEST, \n",
    "            shuffle=True)\n",
    "\n",
    "# For test data\n",
    "test_data_gen = TimeseriesGenerator(x_test_scaled, y_test_scaled, \n",
    "            length=N_LOOKBACK, sampling_rate=1, stride=1, \n",
    "            batch_size=BATCH_SIZE_TEST)\n",
    "\n",
    "# Define all callbacks for model improvance\n",
    "USE_EARLYSTOPPING = True\n",
    "PATIENCE = 20\n",
    "USE_REDUCELR = True\n",
    "\n",
    "path_checkpoint = f'results/model_chekpoints_RNN/{date}_checkpoint.keras'  # path to location for saving checkpoint\n",
    "\n",
    "# Monitor that saves the latest best model regards to validation loss\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                    monitor='val_loss',\n",
    "                                    verbose=1,\n",
    "                                    save_weights_only=True,\n",
    "                                    save_best_only=True)\n",
    "\n",
    "# Early stopping will end an epoch/training if validation does not improve for PATIENCE amount of steps/epochs .\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=PATIENCE,\n",
    "                                    verbose=1)\n",
    "\n",
    "# Reduces learning rate to appropriate number for improved learning\n",
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                        factor=0.1,\n",
    "                                        min_lr=1e-5,\n",
    "                                        patience=2,\n",
    "                                        verbose=1)\n",
    "\n",
    "# List for collecting all callbacks\n",
    "callbacks = []  \n",
    "\n",
    "if USE_EARLYSTOPPING:  # add early stopping if chosen\n",
    "    callbacks.append(callback_early_stopping)\n",
    "    callbacks.append(callback_checkpoint)\n",
    "\n",
    "if USE_REDUCELR:  # add reduceLR if chosen\n",
    "    callbacks.append(callback_reduce_lr)\n",
    "\n",
    "# Compile the model using selected settings\n",
    "model.compile(loss=LOSS,\n",
    "            metrics=METRICS,\n",
    "            optimizer=OPTIMIZER)\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()\n",
    "\n",
    "# Train the model with train set generator, while vaildating against validation data\n",
    "history = model.fit(train_data_gen_shuffle,\n",
    "                            epochs=N_EPOCHS,\n",
    "                            steps_per_epoch=10,\n",
    "                            use_multiprocessing=False,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=val_data_gen,\n",
    "                            verbose=1\n",
    "                            ).history\n",
    "\n",
    "# Reload the best model from PATIENCE amount of epochs earlier\n",
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)\n",
    "\n",
    "# Create and save loss plots\n",
    "ax = pd.DataFrame(history)[['loss', 'val_loss']].plot(logy=True, figsize=(10, 5))\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(str(MODEL_NAME) + ' _losscurve.png')\n",
    "\n",
    "ax = pd.DataFrame(history)[['mae', 'val_mae']].plot(figsize=(10, 5))\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(str(MODEL_NAME) + '_maecurve.png')\n",
    "\n",
    "ax = pd.DataFrame(history)[['mape', 'val_mape']].plot(figsize=(10, 5))\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(str(MODEL_NAME) + '_mapecurve.png')\n",
    "\n",
    "# After training, evaluate and do a final forecast\n",
    "score = model.evaluate(test_data_gen)\n",
    "scores.append(score)\n",
    "\n",
    "trainPredict = model.predict(train_data_gen)\n",
    "valPredict = model.predict(val_data_gen)\n",
    "testPredict = model.predict(test_data_gen)\n",
    "\n",
    "train_predictions.append(trainPredict)\n",
    "val_predictions.append(valPredict)\n",
    "test_predictions.append(testPredict)\n",
    "\n",
    "# Checkpoint save of current prediction arrays as .npy files\n",
    "np.save('results/model_predictions_RNN/'+str(MODEL_NAME)+'_test_predictions', test_predictions)\n",
    "np.save('results/model_predictions_RNN/'+str(MODEL_NAME)+'_train_predictions', train_predictions)\n",
    "np.save('results/model_predictions_RNN/'+str(MODEL_NAME)+'_val_predictions', val_predictions)\n",
    "np.save('results/model_predictions_RNN/'+str(MODEL_NAME)+'_scores', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print evaluation of model on training data generator\n",
    "model.evaluate(train_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of each prediction\n",
    "test_predictions = np.array(test_predictions)\n",
    "train_predictions = np.array(train_predictions)\n",
    "val_predictions = np.array(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of training prediction\n",
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of test prediction\n",
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape prediction arrays\n",
    "train_predictions = train_predictions.reshape(-1, 1)\n",
    "test_predictions = test_predictions.reshape(-1, 1)\n",
    "val_predictions = val_predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print new shape of training prediction\n",
    "train_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print new shape of test prediction\n",
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform prediction results to original scale\n",
    "trainPred = y_scaler.inverse_transform(train_predictions)\n",
    "testPred = y_scaler.inverse_transform(test_predictions)\n",
    "valPred = y_scaler.inverse_transform(val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of inversed training prediction\n",
    "trainPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of inversed test prediction\n",
    "testPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of original feature matrix\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shape of original target array\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save true values to arrays\n",
    "trainTrue = y[0:num_train, 0][-len(trainPred):] # final [] to skip first 168, which are N_LOOKBACK for first prediction in array\n",
    "valTrue = y[num_train:num_train + num_val, 0]\n",
    "testTrue = y[num_train+num_val:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print more shapes\n",
    "trainTrue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valTrue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valPred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTrue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect numeric results and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print seed\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model summary again\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info about settings for current model\n",
    "print(date)\n",
    "print(TRAIN_SIZE, ':', VAL_SIZE, ':', round(TEST_SIZE,1))\n",
    "print('do_share:', DROPOUT_SHARE, 'n_units:', N_UNITS, 'n_layers:', NUM_LAYERS)\n",
    "print('n_epochs:', len(history['val_loss']))\n",
    "print(', n_lb:', N_LOOKBACK, 'bs:', BATCH_SIZE_TRAIN)\n",
    "print(', l:', LOSS, ', opt:', OPTIMIZER, ', act: ', ACTIVATIONS)\n",
    "print('m:', METRICS,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training scores\n",
    "print('MSE = ', round(mean_squared_error(trainTrue,trainPred), 0))\n",
    "print('RMSE = ', round(np.sqrt(mean_squared_error(trainTrue,trainPred)), 2))\n",
    "print('MAE = ', round(mean_absolute_error(trainTrue,trainPred), 2))\n",
    "print('MAPE = ', round(mean_absolute_percentage_error(trainTrue,trainPred)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation scores\n",
    "print('MSE = ', round(mean_squared_error(valTrue,valPred), 0))\n",
    "print('RMSE = ', round(np.sqrt(mean_squared_error(valTrue,valPred)), 2))\n",
    "print('MAE = ', round(mean_absolute_error(valTrue,valPred), 2))\n",
    "print('MAPE = ', round(mean_absolute_percentage_error(valTrue,valPred)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print testing scores\n",
    "print('MSE = ', round(mean_squared_error(testTrue,testPred), 0))\n",
    "print('RMSE = ', round(np.sqrt(mean_squared_error(testTrue,testPred)), 2))\n",
    "print('MAE = ', round(mean_absolute_error(testTrue,testPred), 2))\n",
    "print('MAPE = ', round(mean_absolute_percentage_error(testTrue,testPred)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changeable design parameters\n",
    "figsize = (10, 3)\n",
    "dpi = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of series of training data time stamps from earlier\n",
    "trainsett = list(train_dt[N_LOOKBACK:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length\n",
    "len(trainPred)-len(train_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results on training data\n",
    "plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.plot(trainsett, trainTrue, color='blue', label='mÃ¥lt', linewidth=1, marker=\"o\", ms=1)\n",
    "plt.plot(trainsett, trainPred, color='red', label='predikert')\n",
    "plt.title('Prediksjon av elektrisitetsbruk for hele Oslo lufthavn pÃ¥ treningssett', fontsize=13)\n",
    "plt.xlabel('Dato')\n",
    "plt.ylabel('Timeeffekt [kWh/h]')\n",
    "plt.xticks(trainsett[::450], train_dt[N_LOOKBACK::450].date, rotation=0)\n",
    "plt.ylim(None, max(trainTrue)+200)\n",
    "plt.legend(loc=1, ncol=3, fancybox=True)\n",
    "plt.savefig(f'results/plots_RNN/{date}_train_plott.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of series of test data time stamps from earlier\n",
    "testsett = list(test_dt[N_LOOKBACK:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results on test data\n",
    "plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.plot(testsett, testTrue,color='blue', label='mÃ¥lt', linewidth=1, marker=\"o\", ms=1)\n",
    "plt.plot(testsett, testPred, color='red', label='predikert')\n",
    "plt.title('Prediksjon av elektrisitetsbruk for hele Oslo lufthavn pÃ¥ testsett', fontsize=13)\n",
    "plt.xlabel('Dato')\n",
    "plt.ylabel('Timeeffekt [kWh/h]')\n",
    "plt.xticks(testsett[::60], test_dt[N_LOOKBACK::60].date, rotation=0)\n",
    "plt.ylim(None, max(testTrue)+800)\n",
    "plt.legend(loc=1, ncol=3, fancybox=True)\n",
    "plt.savefig(f'results/plots_RNN/{date}_test_plott.png', bbox_inches='tight')\n",
    "plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing sound/song when script is finished to remember changing setting/parameters and running again\n",
    "from playsound import playsound\n",
    "\n",
    "print(\"Playing Song using playsound\")\n",
    "\n",
    "playsound(\"C://Users//Bruker//Music//01 Area Codes.wma\") # customized to use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dypl-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov  4 2022, 16:35:55) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c2532eeec297a488241587a48a153c1cff7622ce2b66e2b392c1cb74809998a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
